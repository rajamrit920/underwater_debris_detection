# Underwater Debris Detection using YOLO

## Overview
This project focuses on detecting underwater debris using deep learning, specifically leveraging the YOLO (You Only Look Once) object detection model. It aims to assist in marine conservation, pollution monitoring, and underwater research by identifying and classifying underwater objects in real time. The model is designed for efficiency, utilizing preprocessing techniques such as image normalization, noise reduction, and color correction to enhance detection accuracy. 

The dataset consists of annotated underwater images, sourced from publicly available datasets like FathomNet and RUIE, as well as custom-collected images. Training is conducted using a YOLO-based architecture, optimizing performance through precision, recall, mAP (mean Average Precision), and IoU (Intersection over Union) metrics. The system is scalable, supporting large datasets and high-resolution images, while also incorporating explainability methods to enhance transparency in model predictions.

To get started, users must install the required dependencies, configure dataset paths, and train the model using predefined scripts. Testing can be performed on sample images, with results saved in an output directory. Applications of this technology range from marine conservation and underwater security to search and rescue missions and autonomous underwater vehicle (AUV) navigation. 

Future enhancements may include support for multispectral imaging, real-time monitoring capabilities, and integration with underwater robotics for automated debris collection. The project is open-source and welcomes contributions for further development and improvements. 

## Contributors
- **Amrit Raj** ([GitHub](https://github.com/amritraj))
- **Snehasish Kabi** # Underwater Debris Detection using YOLO

## Overview
This project focuses on detecting underwater debris using deep learning, specifically leveraging the YOLO (You Only Look Once) object detection model. It aims to assist in marine conservation, pollution monitoring, and underwater research by identifying and classifying underwater objects in real time. The model is designed for efficiency, utilizing preprocessing techniques such as image normalization, noise reduction, and color correction to enhance detection accuracy. 

The dataset consists of annotated underwater images, sourced from publicly available datasets like FathomNet and RUIE, as well as custom-collected images. Training is conducted using a YOLO-based architecture, optimizing performance through precision, recall, mAP (mean Average Precision), and IoU (Intersection over Union) metrics. The system is scalable, supporting large datasets and high-resolution images, while also incorporating explainability methods to enhance transparency in model predictions.

To get started, users must install the required dependencies, configure dataset paths, and train the model using predefined scripts. Testing can be performed on sample images, with results saved in an output directory. Applications of this technology range from marine conservation and underwater security to search and rescue missions and autonomous underwater vehicle (AUV) navigation. 

Future enhancements may include support for multispectral imaging, real-time monitoring capabilities, and integration with underwater robotics for automated debris collection. The project is open-source and welcomes contributions for further development and improvements. 

## Contributors
- **Amrit Raj** ([GitHub](https://github.com/amritraj))
- **Snehasish Kabi** ([GitHub](https://github.com/snehasishkabi))

## License
This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments
- BMS College of Engineering
- Visvesvaraya Technological University
- Our project guide, Prof. Shruthi K R



## License
This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments
- BMS College of Engineering
- Visvesvaraya Technological University
- Our project guide, Prof. Shruthi K R

